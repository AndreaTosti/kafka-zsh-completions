#!/bin/sh
#
# DISCLAIMER: THIS FILE HAS BEEN AUTOMATICALLY GENERATED
# PLEASE DO NOT TOUCH!!!
# IF YOU NEED TO DO ANY MODIFICATION, EDIT GENERATE.ZSH
# FOR MORE INFORMATION https://github.com/Dabz/kafka-zsh-completions
#

function _kafka-command() {
	cmd=$1
	arg_name="$(echo $cmd | tr - _)_args"
	typeset -a options
	eval _describe 'values' options -- $arg_name
}

declare -a kafka_acls_args
kafka_acls_args=()
kafka_acls_args+=('--add: Indicates you are trying to add ACLs.')
kafka_acls_args+=('--allow-host: <String: allow-host> Host from which principals listed in -- allow-principal will have access. If you have specified')
kafka_acls_args+=('--allow-principal: then the default for this option will be set to * which allows access from all hosts.')
kafka_acls_args+=('--allow-principal: <String: allow- principal is in principalType:name principal> format. Note that principalType must be supported by the Authorizer being used. For example, User:* is the wild card indicating all users.')
kafka_acls_args+=('--authorizer: <String: authorizer> Fully qualified class name of the authorizer, defaults to kafka. security.auth.SimpleAclAuthorizer. (default: kafka.security.auth. SimpleAclAuthorizer)')
kafka_acls_args+=('--authorizer-properties: <String: REQUIRED: properties required to authorizer-properties> configure an instance of Authorizer. These are key=val pairs. For the default authorizer the example values are: zookeeper. connect=localhost:2181')
kafka_acls_args+=('--cluster: Add/Remove cluster ACLs.')
kafka_acls_args+=('--consumer: Convenience option to add/remove ACLs for consumer role. This will generate ACLs that allows READ, DESCRIBE on topic and READ on group.')
kafka_acls_args+=('--delegation-token: <String: delegation- Delegation token to which ACLs should token> be added or removed. A value of * indicates ACL should apply to all tokens.')
kafka_acls_args+=('--deny-host: <String: deny-host> Host from which principals listed in -- deny-principal will be denied access. If you have specified')
kafka_acls_args+=('--deny-: principal then the default for this option will be set to * which denies access from all hosts.')
kafka_acls_args+=('--deny-principal: <String: deny- principal is in principalType:name principal> format. By default anyone not added through')
kafka_acls_args+=('--allow-principal: is denied access. You only need to use this option as negation to already allowed set. Note that principalType must be supported by the Authorizer being used. For example if you wanted to allow access to all users in the system but not test-user you can define an ACL that allows access to User:* and specify')
kafka_acls_args+=('--deny-: principal=User:test@EXAMPLE.COM. AND PLEASE REMEMBER DENY RULES TAKES PRECEDENCE OVER ALLOW RULES.')
kafka_acls_args+=('--force: Assume Yes to all queries and do not prompt.')
kafka_acls_args+=('--group: <String: group> Consumer Group to which the ACLs should be added or removed. A value of * indicates the ACLs should apply to all groups.')
kafka_acls_args+=('--help: Print usage information.')
kafka_acls_args+=('--idempotent: Enable idempotence for the producer. This should be used in combination with the')
kafka_acls_args+=('--producer: option. Note that idempotence is enabled automatically if the producer is authorized to a particular transactional-id.')
kafka_acls_args+=('--list: List ACLs for the specified resource, use')
kafka_acls_args+=('--topic: <topic> or')
kafka_acls_args+=('--group: <group> or')
kafka_acls_args+=('--cluster: to specify a resource.')
kafka_acls_args+=('--operation: <String> Operation that is being allowed or denied. Valid operation names are: Read Write Create Delete Alter Describe ClusterAction AlterConfigs DescribeConfigs IdempotentWrite All (default: All)')
kafka_acls_args+=('--producer: Convenience option to add/remove ACLs for producer role. This will generate ACLs that allows WRITE, DESCRIBE and CREATE on topic.')
kafka_acls_args+=('--remove: Indicates you are trying to remove ACLs.')
kafka_acls_args+=('--resource-pattern-type: The type of the resource pattern or <ANY|MATCH|LITERAL|PREFIXED> pattern filter. When adding acls, this should be a specific pattern type, e.g. ''literal'' or ''prefixed''. When listing or removing acls, a specific pattern type can be used to list or remove acls from specific resource patterns, or use the filter values of ''any'' or ''match'', where ''any'' will match any pattern type, but will match the resource name exactly, where as ''match'' will perform pattern matching to list or remove all acls that affect the supplied resource(s). WARNING: ''match'', when used in combination with the ''--remove'' switch, should be used with care. (default: LITERAL)')
kafka_acls_args+=('--topic: <String: topic> topic to which ACLs should be added or removed. A value of * indicates ACL should apply to all topics.')
kafka_acls_args+=('--transactional-id: <String: The transactionalId to which ACLs transactional-id> should be added or removed. A value of * indicates the ACLs should apply to all transactionalIds.')
compdef "_kafka-command kafka-acls" kafka-acls
declare -a kafka_avro_console_consumer_args
kafka_avro_console_consumer_args=()
kafka_avro_console_consumer_args+=('--bootstrap-server: <String: server to REQUIRED: The server(s) to connect to. connect to>')
kafka_avro_console_consumer_args+=('--consumer-property: <String: A mechanism to pass user-defined consumer_prop> properties in the form key=value to the consumer.')
kafka_avro_console_consumer_args+=('--consumer.config: <String: config file> Consumer config properties file. Note that [consumer-property] takes precedence over this config.')
kafka_avro_console_consumer_args+=('--enable-systest-events: Log lifecycle events of the consumer in addition to logging consumed messages. (This is specific for system tests.)')
kafka_avro_console_consumer_args+=('--formatter: <String: class> The name of a class to use for formatting kafka messages for display. (default: kafka.tools. DefaultMessageFormatter)')
kafka_avro_console_consumer_args+=('--from-beginning: If the consumer does not already have an established offset to consume from, start with the earliest message present in the log rather than the latest message.')
kafka_avro_console_consumer_args+=('--group: <String: consumer group id> The consumer group id of the consumer.')
kafka_avro_console_consumer_args+=('--isolation-level: <String> Set to read_committed in order to filter out transactional messages which are not committed. Set to read_uncommittedto read all messages. (default: read_uncommitted)')
kafka_avro_console_consumer_args+=('--key-deserializer: <String: deserializer for key>')
kafka_avro_console_consumer_args+=('--max-messages: <Integer: num_messages> The maximum number of messages to consume before exiting. If not set, consumption is continual.')
kafka_avro_console_consumer_args+=('--offset: <String: consume offset> The offset id to consume from (a non- negative number), or ''earliest'' which means from beginning, or ''latest'' which means from end (default: latest)')
kafka_avro_console_consumer_args+=('--partition: <Integer: partition> The partition to consume from. Consumption starts from the end of the partition unless ''--offset'' is specified.')
kafka_avro_console_consumer_args+=('--property: <String: prop> The properties to initialize the message formatter. Default properties include: print.timestamp=true|false print.key=true|false print.value=true|false key.separator=<key.separator> line.separator=<line.separator> key.deserializer=<key.deserializer> value.deserializer=<value. deserializer> Users can also pass in customized properties for their formatter; more specifically, users can pass in properties keyed with ''key. deserializer.'' and ''value. deserializer.'' prefixes to configure their deserializers.')
kafka_avro_console_consumer_args+=('--skip-message-on-error: If there is an error when processing a message, skip it instead of halt.')
kafka_avro_console_consumer_args+=('--timeout-ms: <Integer: timeout_ms> If specified, exit if no message is available for consumption for the specified interval.')
kafka_avro_console_consumer_args+=('--topic: <String: topic> The topic id to consume on.')
kafka_avro_console_consumer_args+=('--value-deserializer: <String: deserializer for values>')
kafka_avro_console_consumer_args+=('--whitelist: <String: whitelist> Whitelist of topics to include for consumption.')
compdef "_kafka-command kafka-avro-console-consumer" kafka-avro-console-consumer
declare -a kafka_avro_console_producer_args
kafka_avro_console_producer_args=()
kafka_avro_console_producer_args+=('--batch-size: <Integer: size> Number of messages to send in a single batch if they are not being sent synchronously. (default: 200)')
kafka_avro_console_producer_args+=('--broker-list: <String: broker-list> REQUIRED: The broker list string in the form HOST1:PORT1,HOST2:PORT2.')
kafka_avro_console_producer_args+=('--compression-codec: [String: The compression codec: either ''none'', compression-codec] ''gzip'', ''snappy'', or ''lz4''.If specified without value, then it defaults to ''gzip''')
kafka_avro_console_producer_args+=('--line-reader: <String: reader_class> The class name of the class to use for reading lines from standard in. By default each line is read as a separate message. (default: kafka. tools. ConsoleProducer$LineMessageReader)')
kafka_avro_console_producer_args+=('--max-block-ms: <Long: max block on The max time that the producer will send> block for during a send request (default: 60000)')
kafka_avro_console_producer_args+=('--max-memory-bytes: <Long: total memory The total memory used by the producer in bytes> to buffer records waiting to be sent to the server. (default: 33554432)')
kafka_avro_console_producer_args+=('--max-partition-memory-bytes: <Long: The buffer size allocated for a memory in bytes per partition> partition. When records are received which are smaller than this size the producer will attempt to optimistically group them together until this size is reached. (default: 16384)')
kafka_avro_console_producer_args+=('--message-send-max-retries: <Integer> Brokers can fail receiving the message for multiple reasons, and being unavailable transiently is just one of them. This property specifies the number of retires before the producer give up and drop this message. (default: 3)')
kafka_avro_console_producer_args+=('--metadata-expiry-ms: <Long: metadata The period of time in milliseconds expiration interval> after which we force a refresh of metadata even if we haven''t seen any leadership changes. (default: 300000)')
kafka_avro_console_producer_args+=('--producer-property: <String: A mechanism to pass user-defined producer_prop> properties in the form key=value to the producer.')
kafka_avro_console_producer_args+=('--producer.config: <String: config file> Producer config properties file. Note that [producer-property] takes precedence over this config.')
kafka_avro_console_producer_args+=('--property: <String: prop> A mechanism to pass user-defined properties in the form key=value to the message reader. This allows custom configuration for a user- defined message reader.')
kafka_avro_console_producer_args+=('--request-required-acks: <String: The required acks of the producer request required acks> requests (default: 1)')
kafka_avro_console_producer_args+=('--request-timeout-ms: <Integer: request The ack timeout of the producer timeout ms> requests. Value must be non-negative and non-zero (default: 1500)')
kafka_avro_console_producer_args+=('--retry-backoff-ms: <Integer> Before each retry, the producer refreshes the metadata of relevant topics. Since leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata. (default: 100)')
kafka_avro_console_producer_args+=('--socket-buffer-size: <Integer: size> The size of the tcp RECV size. (default: 102400)')
kafka_avro_console_producer_args+=('--sync: If set message send requests to the brokers are synchronously, one at a time as they arrive.')
kafka_avro_console_producer_args+=('--timeout: <Integer: timeout_ms> If set and the producer is running in asynchronous mode, this gives the maximum amount of time a message will queue awaiting sufficient batch size. The value is given in ms. (default: 1000)')
kafka_avro_console_producer_args+=('--topic: <String: topic> REQUIRED: The topic id to produce messages to.')
compdef "_kafka-command kafka-avro-console-producer" kafka-avro-console-producer
declare -a kafka_broker_api_versions_args
kafka_broker_api_versions_args=()
kafka_broker_api_versions_args+=('--bootstrap-server: <String: server(s) REQUIRED: The server to connect to. to use for bootstrapping>')
kafka_broker_api_versions_args+=('--command-config: <String: command A property file containing configs to config property file> be passed to Admin Client.')
compdef "_kafka-command kafka-broker-api-versions" kafka-broker-api-versions
declare -a kafka_configs_args
kafka_configs_args=()
kafka_configs_args+=('--add-config: <String> Key Value pairs of configs to add. Square brackets can be used to group values which contain commas: ''k1=v1, k2=[v1,v2,v2],k3=v3''. The following is a list of valid configurations: For entity-type ''topics'': cleanup.policy compression.type delete.retention.ms file.delete.delay.ms flush.messages flush.ms follower.replication.throttled. replicas index.interval.bytes leader.replication.throttled.replicas max.message.bytes message.downconversion.enable message.format.version message.timestamp.difference.max.ms message.timestamp.type min.cleanable.dirty.ratio min.compaction.lag.ms min.insync.replicas preallocate retention.bytes retention.ms segment.bytes segment.index.bytes segment.jitter.ms segment.ms unclean.leader.election.enable For entity-type ''brokers'': log.message.timestamp.type ssl.client.auth log.retention.ms sasl.login.refresh.window.jitter sasl.kerberos.ticket.renew.window. factor log.preallocate log.index.size.max.bytes sasl.login.refresh.window.factor ssl.truststore.type ssl.keymanager.algorithm log.cleaner.io.buffer.load.factor sasl.login.refresh.min.period.seconds ssl.key.password background.threads log.retention.bytes ssl.trustmanager.algorithm log.segment.bytes log.cleaner.delete.retention.ms log.segment.delete.delay.ms min.insync.replicas ssl.keystore.location ssl.cipher.suites log.roll.jitter.ms log.cleaner.backoff.ms sasl.jaas.config principal.builder.class log.flush.interval.ms log.cleaner.dedupe.buffer.size log.flush.interval.messages advertised.listeners num.io.threads listener.security.protocol.map log.message.downconversion.enable sasl.enabled.mechanisms sasl.login.refresh.buffer.seconds ssl.truststore.password listeners metric.reporters ssl.protocol sasl.kerberos.ticket.renew.jitter ssl.keystore.password sasl.mechanism.inter.broker.protocol log.cleanup.policy sasl.kerberos.principal.to.local.rules sasl.kerberos.min.time.before.relogin num.recovery.threads.per.data.dir log.cleaner.io.max.bytes.per.second log.roll.ms ssl.endpoint.identification.algorithm unclean.leader.election.enable message.max.bytes log.cleaner.threads log.cleaner.io.buffer.size sasl.kerberos.service.name ssl.provider follower.replication.throttled.rate log.index.interval.bytes log.cleaner.min.compaction.lag.ms log.message.timestamp.difference.max. ms ssl.enabled.protocols log.cleaner.min.cleanable.ratio replica.alter.log.dirs.io.max.bytes. per.second ssl.keystore.type ssl.secure.random.implementation ssl.truststore.location sasl.kerberos.kinit.cmd leader.replication.throttled.rate num.network.threads compression.type num.replica.fetchers For entity-type ''users'': request_percentage producer_byte_rate SCRAM-SHA-256 SCRAM-SHA-512 consumer_byte_rate For entity-type ''clients'': request_percentage producer_byte_rate consumer_byte_rate Entity types ''users'' and ''clients'' may be specified together to update config for clients of a specific user.')
kafka_configs_args+=('--alter: Alter the configuration for the entity.')
kafka_configs_args+=('--bootstrap-server: <String: server to The Kafka server to connect to. This connect to> is required for describing and altering broker configs.')
kafka_configs_args+=('--command-config: <String: command Property file containing configs to be config property file> passed to Admin Client. This is used only with')
kafka_configs_args+=('--bootstrap-server: option for describing and altering broker configs.')
kafka_configs_args+=('--delete-config: <String> config keys to remove ''k1,k2''')
kafka_configs_args+=('--describe: List configs for the given entity.')
kafka_configs_args+=('--entity-default: Default entity name for clients/users/brokers (applies to corresponding entity type in command line)')
kafka_configs_args+=('--entity-name: <String> Name of entity (topic name/client id/user principal name/broker id)')
kafka_configs_args+=('--entity-type: <String> Type of entity (topics/clients/users/brokers)')
kafka_configs_args+=('--force: Suppress console prompts')
kafka_configs_args+=('--help: Print usage information.')
kafka_configs_args+=('--zookeeper: <String: urls> REQUIRED: The connection string for the zookeeper connection in the form host:port. Multiple URLS can be given to allow fail-over.')
compdef "_kafka-command kafka-configs" kafka-configs
declare -a kafka_console_consumer_args
kafka_console_consumer_args=()
kafka_console_consumer_args+=('--bootstrap-server: <String: server to REQUIRED: The server(s) to connect to. connect to>')
kafka_console_consumer_args+=('--consumer-property: <String: A mechanism to pass user-defined consumer_prop> properties in the form key=value to the consumer.')
kafka_console_consumer_args+=('--consumer.config: <String: config file> Consumer config properties file. Note that [consumer-property] takes precedence over this config.')
kafka_console_consumer_args+=('--enable-systest-events: Log lifecycle events of the consumer in addition to logging consumed messages. (This is specific for system tests.)')
kafka_console_consumer_args+=('--formatter: <String: class> The name of a class to use for formatting kafka messages for display. (default: kafka.tools. DefaultMessageFormatter)')
kafka_console_consumer_args+=('--from-beginning: If the consumer does not already have an established offset to consume from, start with the earliest message present in the log rather than the latest message.')
kafka_console_consumer_args+=('--group: <String: consumer group id> The consumer group id of the consumer.')
kafka_console_consumer_args+=('--isolation-level: <String> Set to read_committed in order to filter out transactional messages which are not committed. Set to read_uncommittedto read all messages. (default: read_uncommitted)')
kafka_console_consumer_args+=('--key-deserializer: <String: deserializer for key>')
kafka_console_consumer_args+=('--max-messages: <Integer: num_messages> The maximum number of messages to consume before exiting. If not set, consumption is continual.')
kafka_console_consumer_args+=('--offset: <String: consume offset> The offset id to consume from (a non- negative number), or ''earliest'' which means from beginning, or ''latest'' which means from end (default: latest)')
kafka_console_consumer_args+=('--partition: <Integer: partition> The partition to consume from. Consumption starts from the end of the partition unless ''--offset'' is specified.')
kafka_console_consumer_args+=('--property: <String: prop> The properties to initialize the message formatter. Default properties include: print.timestamp=true|false print.key=true|false print.value=true|false key.separator=<key.separator> line.separator=<line.separator> key.deserializer=<key.deserializer> value.deserializer=<value. deserializer> Users can also pass in customized properties for their formatter; more specifically, users can pass in properties keyed with ''key. deserializer.'' and ''value. deserializer.'' prefixes to configure their deserializers.')
kafka_console_consumer_args+=('--skip-message-on-error: If there is an error when processing a message, skip it instead of halt.')
kafka_console_consumer_args+=('--timeout-ms: <Integer: timeout_ms> If specified, exit if no message is available for consumption for the specified interval.')
kafka_console_consumer_args+=('--topic: <String: topic> The topic id to consume on.')
kafka_console_consumer_args+=('--value-deserializer: <String: deserializer for values>')
kafka_console_consumer_args+=('--whitelist: <String: whitelist> Whitelist of topics to include for consumption.')
compdef "_kafka-command kafka-console-consumer" kafka-console-consumer
declare -a kafka_console_producer_args
kafka_console_producer_args=()
kafka_console_producer_args+=('--batch-size: <Integer: size> Number of messages to send in a single batch if they are not being sent synchronously. (default: 200)')
kafka_console_producer_args+=('--broker-list: <String: broker-list> REQUIRED: The broker list string in the form HOST1:PORT1,HOST2:PORT2.')
kafka_console_producer_args+=('--compression-codec: [String: The compression codec: either ''none'', compression-codec] ''gzip'', ''snappy'', or ''lz4''.If specified without value, then it defaults to ''gzip''')
kafka_console_producer_args+=('--line-reader: <String: reader_class> The class name of the class to use for reading lines from standard in. By default each line is read as a separate message. (default: kafka. tools. ConsoleProducer$LineMessageReader)')
kafka_console_producer_args+=('--max-block-ms: <Long: max block on The max time that the producer will send> block for during a send request (default: 60000)')
kafka_console_producer_args+=('--max-memory-bytes: <Long: total memory The total memory used by the producer in bytes> to buffer records waiting to be sent to the server. (default: 33554432)')
kafka_console_producer_args+=('--max-partition-memory-bytes: <Long: The buffer size allocated for a memory in bytes per partition> partition. When records are received which are smaller than this size the producer will attempt to optimistically group them together until this size is reached. (default: 16384)')
kafka_console_producer_args+=('--message-send-max-retries: <Integer> Brokers can fail receiving the message for multiple reasons, and being unavailable transiently is just one of them. This property specifies the number of retires before the producer give up and drop this message. (default: 3)')
kafka_console_producer_args+=('--metadata-expiry-ms: <Long: metadata The period of time in milliseconds expiration interval> after which we force a refresh of metadata even if we haven''t seen any leadership changes. (default: 300000)')
kafka_console_producer_args+=('--producer-property: <String: A mechanism to pass user-defined producer_prop> properties in the form key=value to the producer.')
kafka_console_producer_args+=('--producer.config: <String: config file> Producer config properties file. Note that [producer-property] takes precedence over this config.')
kafka_console_producer_args+=('--property: <String: prop> A mechanism to pass user-defined properties in the form key=value to the message reader. This allows custom configuration for a user- defined message reader.')
kafka_console_producer_args+=('--request-required-acks: <String: The required acks of the producer request required acks> requests (default: 1)')
kafka_console_producer_args+=('--request-timeout-ms: <Integer: request The ack timeout of the producer timeout ms> requests. Value must be non-negative and non-zero (default: 1500)')
kafka_console_producer_args+=('--retry-backoff-ms: <Integer> Before each retry, the producer refreshes the metadata of relevant topics. Since leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata. (default: 100)')
kafka_console_producer_args+=('--socket-buffer-size: <Integer: size> The size of the tcp RECV size. (default: 102400)')
kafka_console_producer_args+=('--sync: If set message send requests to the brokers are synchronously, one at a time as they arrive.')
kafka_console_producer_args+=('--timeout: <Integer: timeout_ms> If set and the producer is running in asynchronous mode, this gives the maximum amount of time a message will queue awaiting sufficient batch size. The value is given in ms. (default: 1000)')
kafka_console_producer_args+=('--topic: <String: topic> REQUIRED: The topic id to produce messages to.')
compdef "_kafka-command kafka-console-producer" kafka-console-producer
declare -a kafka_consumer_groups_args
kafka_consumer_groups_args=()
kafka_consumer_groups_args+=('--all-topics: Consider all topics assigned to a group in the `reset-offsets` process.')
kafka_consumer_groups_args+=('--bootstrap-server: <String: server to REQUIRED: The server(s) to connect to. connect to>')
kafka_consumer_groups_args+=('--by-duration: <String: duration> Reset offsets to offset by duration from current timestamp. Format: ''PnDTnHnMnS''')
kafka_consumer_groups_args+=('--command-config: <String: command Property file containing configs to be config property file> passed to Admin Client and Consumer.')
kafka_consumer_groups_args+=('--delete: Pass in groups to delete topic partition offsets and ownership information over the entire consumer group. For instance')
kafka_consumer_groups_args+=('--group: g1 -- group g2')
kafka_consumer_groups_args+=('--describe: Describe consumer group and list offset lag (number of messages not yet processed) related to given group.')
kafka_consumer_groups_args+=('--dry-run: Only show results without executing changes on Consumer Groups. Supported operations: reset-offsets.')
kafka_consumer_groups_args+=('--execute: Execute operation. Supported operations: reset-offsets.')
kafka_consumer_groups_args+=('--export: Export operation execution to a CSV file. Supported operations: reset- offsets.')
kafka_consumer_groups_args+=('--from-file: <String: path to CSV file> Reset offsets to values defined in CSV file.')
kafka_consumer_groups_args+=('--group: <String: consumer group> The consumer group we wish to act on.')
kafka_consumer_groups_args+=('--list: List all consumer groups.')
kafka_consumer_groups_args+=('--members: Describe members of the group. This option may be used with ''--describe'' and ''--bootstrap-server'' options only. Example:')
kafka_consumer_groups_args+=('--bootstrap-server: localhost: 9092')
kafka_consumer_groups_args+=('--describe:')
kafka_consumer_groups_args+=('--group: group1 -- members')
kafka_consumer_groups_args+=('--offsets: Describe the group and list all topic partitions in the group along with their offset lag. This is the default sub-action of and may be used with ''--describe'' and ''-- bootstrap-server'' options only. Example:')
kafka_consumer_groups_args+=('--bootstrap-server: localhost: 9092')
kafka_consumer_groups_args+=('--describe:')
kafka_consumer_groups_args+=('--group: group1 -- offsets')
kafka_consumer_groups_args+=('--reset-offsets: Reset offsets of consumer group. Supports one consumer group at the time, and instances should be inactive Has 2 execution options:')
kafka_consumer_groups_args+=('--dry-run: (the default) to plan which offsets to reset, and')
kafka_consumer_groups_args+=('--execute: to update the offsets. Additionally, the -- export option is used to export the results to a CSV format. You must choose one of the following reset specifications:')
kafka_consumer_groups_args+=('--to-datetime,:')
kafka_consumer_groups_args+=('--by-period,:')
kafka_consumer_groups_args+=('--to-earliest,:')
kafka_consumer_groups_args+=('--to-: latest,')
kafka_consumer_groups_args+=('--shift-by,:')
kafka_consumer_groups_args+=('--from-file,: -- to-current. To define the scope use')
kafka_consumer_groups_args+=('--all-topics: or')
kafka_consumer_groups_args+=('--topic.: One scope must be specified unless you use ''--from- file''.')
kafka_consumer_groups_args+=('--shift-by: <Long: number-of-offsets> Reset offsets shifting current offset by ''n'', where ''n'' can be positive or negative.')
kafka_consumer_groups_args+=('--state: Describe the group state. This option may be used with ''--describe'' and ''-- bootstrap-server'' options only. Example:')
kafka_consumer_groups_args+=('--bootstrap-server: localhost: 9092')
kafka_consumer_groups_args+=('--describe:')
kafka_consumer_groups_args+=('--group: group1 -- state')
kafka_consumer_groups_args+=('--timeout: <Long: timeout (ms)> The timeout that can be set for some use cases. For example, it can be used when describing the group to specify the maximum amount of time in milliseconds to wait before the group stabilizes (when the group is just created, or is going through some changes). (default: 5000)')
kafka_consumer_groups_args+=('--to-current: Reset offsets to current offset.')
kafka_consumer_groups_args+=('--to-datetime: <String: datetime> Reset offsets to offset from datetime. Format: ''YYYY-MM-DDTHH:mm:SS.sss''')
kafka_consumer_groups_args+=('--to-earliest: Reset offsets to earliest offset.')
kafka_consumer_groups_args+=('--to-latest: Reset offsets to latest offset.')
kafka_consumer_groups_args+=('--to-offset: <Long: offset> Reset offsets to a specific offset.')
kafka_consumer_groups_args+=('--topic: <String: topic> The topic whose consumer group information should be deleted or topic whose should be included in the reset offset process. In `reset- offsets` case, partitions can be specified using this format: `topic1: 0,1,2`, where 0,1,2 are the partition to be included in the process. Reset-offsets also supports multiple topic inputs.')
kafka_consumer_groups_args+=('--verbose: Provide additional information, if any, when describing the group. This option may be used with ''-- offsets''/''--members''/''--state'' and ''--bootstrap-server'' options only. Example:')
kafka_consumer_groups_args+=('--bootstrap-server: localhost: 9092')
kafka_consumer_groups_args+=('--describe:')
kafka_consumer_groups_args+=('--group: group1 -- members')
kafka_consumer_groups_args+=('--verbose:')
compdef "_kafka-command kafka-consumer-groups" kafka-consumer-groups
declare -a kafka_consumer_perf_test_args
kafka_consumer_perf_test_args=()
kafka_consumer_perf_test_args+=('--broker-list: <String: host> REQUIRED: The server(s) to connect to.')
kafka_consumer_perf_test_args+=('--consumer.config: <String: config file> Consumer config properties file.')
kafka_consumer_perf_test_args+=('--date-format: <String: date format> The date format to use for formatting the time field. See java.text. SimpleDateFormat for options. (default: yyyy-MM-dd HH:mm:ss:SSS)')
kafka_consumer_perf_test_args+=('--fetch-size: <Integer: size> The amount of data to fetch in a single request. (default: 1048576)')
kafka_consumer_perf_test_args+=('--from-latest: If the consumer does not already have an established offset to consume from, start with the latest message present in the log rather than the earliest message.')
kafka_consumer_perf_test_args+=('--group: <String: gid> The group id to consume on. (default: perf-consumer-20667)')
kafka_consumer_perf_test_args+=('--help: Print usage.')
kafka_consumer_perf_test_args+=('--hide-header: If set, skips printing the header for the stats')
kafka_consumer_perf_test_args+=('--messages: <Long: count> REQUIRED: The number of messages to send or consume')
kafka_consumer_perf_test_args+=('--num-fetch-threads: <Integer: count> Number of fetcher threads. (default: 1)')
kafka_consumer_perf_test_args+=('--print-metrics: Print out the metrics.')
kafka_consumer_perf_test_args+=('--reporting-interval: <Integer: Interval in milliseconds at which to interval_ms> print progress info. (default: 5000)')
kafka_consumer_perf_test_args+=('--show-detailed-stats: If set, stats are reported for each reporting interval as configured by reporting-interval')
kafka_consumer_perf_test_args+=('--socket-buffer-size: <Integer: size> The size of the tcp RECV size. (default: 2097152)')
kafka_consumer_perf_test_args+=('--threads: <Integer: count> Number of processing threads. (default: 10)')
kafka_consumer_perf_test_args+=('--timeout: [Long: milliseconds] The maximum allowed time in milliseconds between returned records. (default: 10000)')
kafka_consumer_perf_test_args+=('--topic: <String: topic> REQUIRED: The topic to consume from.')
compdef "_kafka-command kafka-consumer-perf-test" kafka-consumer-perf-test
declare -a kafka_delegation_tokens_args
kafka_delegation_tokens_args=()
kafka_delegation_tokens_args+=('--bootstrap-server: <String> REQUIRED: server(s) to use for bootstrapping.')
kafka_delegation_tokens_args+=('--command-config: <String> REQUIRED: A property file containing configs to be passed to Admin Client. Token management operations are allowed in secure mode only. This config file is used to pass security related configs.')
kafka_delegation_tokens_args+=('--create: Create a new delegation token. Use')
kafka_delegation_tokens_args+=('--renewer-: principal option to pass renewers principals.')
kafka_delegation_tokens_args+=('--describe: Describe delegation tokens for the given principals. Use')
kafka_delegation_tokens_args+=('--owner-principal: to pass owner/renewer principals. If')
kafka_delegation_tokens_args+=('--owner-principal: option is not supplied, all the user owned tokens and tokens where user have Describe permission will be returned.')
kafka_delegation_tokens_args+=('--expire: Expire delegation token. Use')
kafka_delegation_tokens_args+=('--expiry-time-: period option to expire the token.')
kafka_delegation_tokens_args+=('--expiry-time-period: [Long] Expiry time period in milliseconds. If the value is -1, then the token will get invalidated immediately.')
kafka_delegation_tokens_args+=('--hmac: [String] HMAC of the delegation token')
kafka_delegation_tokens_args+=('--max-life-time-period: [Long] Max life period for the token in milliseconds. If the value is -1, then token max life time will default to a server side config value (delegation.token.max.lifetime.ms).')
kafka_delegation_tokens_args+=('--owner-principal: [String] owner is a kafka principal. It is should be in principalType:name format.')
kafka_delegation_tokens_args+=('--renew: Renew delegation token. Use')
kafka_delegation_tokens_args+=('--renew-time-period: option to set renew time period.')
kafka_delegation_tokens_args+=('--renew-time-period: [Long] Renew time period in milliseconds. If the value is -1, then the renew time period will default to a server side config value (delegation. token.expiry.time.ms).')
kafka_delegation_tokens_args+=('--renewer-principal: [String] renewer is a kafka principal. It is should be in principalType:name format.')
compdef "_kafka-command kafka-delegation-tokens" kafka-delegation-tokens
declare -a kafka_topics_args
kafka_topics_args=()
kafka_topics_args+=('--alter: Alter the number of partitions, replica assignment, and/or configuration for the topic.')
kafka_topics_args+=('--config: <String: name=value> A topic configuration override for the topic being created or altered.The following is a list of valid configurations: cleanup.policy compression.type delete.retention.ms file.delete.delay.ms flush.messages flush.ms follower.replication.throttled. replicas index.interval.bytes leader.replication.throttled.replicas max.message.bytes message.downconversion.enable message.format.version message.timestamp.difference.max.ms message.timestamp.type min.cleanable.dirty.ratio min.compaction.lag.ms min.insync.replicas preallocate retention.bytes retention.ms segment.bytes segment.index.bytes segment.jitter.ms segment.ms unclean.leader.election.enable See the Kafka documentation for full details on the topic configs.')
kafka_topics_args+=('--create: Create a new topic.')
kafka_topics_args+=('--delete: Delete a topic')
kafka_topics_args+=('--delete-config: <String: name> A topic configuration override to be removed for an existing topic (see the list of configurations under the')
kafka_topics_args+=('--config: option).')
kafka_topics_args+=('--describe: List details for the given topics.')
kafka_topics_args+=('--disable-rack-aware: Disable rack aware replica assignment')
kafka_topics_args+=('--force: Suppress console prompts')
kafka_topics_args+=('--help: Print usage information.')
kafka_topics_args+=('--if-exists: if set when altering or deleting topics, the action will only execute if the topic exists')
kafka_topics_args+=('--if-not-exists: if set when creating topics, the action will only execute if the topic does not already exist')
kafka_topics_args+=('--list: List all available topics.')
kafka_topics_args+=('--partitions: <Integer: # of partitions> The number of partitions for the topic being created or altered (WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected')
kafka_topics_args+=('--replica-assignment: <String: A list of manual partition-to-broker broker_id_for_part1_replica1 : assignments for the topic being broker_id_for_part1_replica2 , created or altered. broker_id_for_part2_replica1 : broker_id_for_part2_replica2 , ...>')
kafka_topics_args+=('--replication-factor: <Integer: The replication factor for each replication factor> partition in the topic being created.')
kafka_topics_args+=('--topic: <String: topic> The topic to be create, alter or describe. Can also accept a regular expression except for')
kafka_topics_args+=('--create: option')
kafka_topics_args+=('--topics-with-overrides: if set when describing topics, only show topics that have overridden configs')
kafka_topics_args+=('--unavailable-partitions: if set when describing topics, only show partitions whose leader is not available')
kafka_topics_args+=('--under-replicated-partitions: if set when describing topics, only show under replicated partitions')
kafka_topics_args+=('--zookeeper: <String: hosts> REQUIRED: The connection string for the zookeeper connection in the form host:port. Multiple hosts can be given to allow fail-over.')
compdef "_kafka-command kafka-topics" kafka-topics
